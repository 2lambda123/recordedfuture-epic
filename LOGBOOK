

9/22/2011:

Exact training, 2 parsers, 2 states:
decoding:
Exact: length 40: 83.6
ADF: 82.57
EP: 83.55
The first parser: 69.5
The second parser: 48.67
Product: 72.40


9/7/2011:

ADF: 3 parsers, len 40: 83.2
EP: 2 parsers, relaunch from earlier 2 parser run: 83.4 (init from ADF), 83.0 (init from EP)

9/6/2011:


Two Len 40, sub-state parsers, 80.5 and 80.8 F1

Product, product of Variational Decoding: 77.6
Product, product of max rule decoder: 81.26
ADF, Last one decoder max constituent: 78.6
EP, last one decoder max constituent: 80.45
*ADF, initialized from above: 83.5
*ADF, initialized with 1 of the above for f1: 82.9


9/4/2011:

Trained on ADF, Len 40:
EP-0 == ADF: 82.5
EP-*: 82.5


F0: 80.9
Just use f1 parser: 81.0
Just use f2 parser: 47.6

It does matter if you decode with f1 or f2 (81, versus 82.5)

Product: 77.6


9/3/2011:

Trained on EP, Len 40.
EP-0 == ADF : 82.0
EP-1: 82.0
EP-2: 82.0
EP-3: 82.0
EP-4: 82.0

EP-4, f0: 80.5

Just use the f1 parser: 59.71
Just use the f2 parser: 63.34
Product Parser: 77.76

Doesnt' matter if you decode with f1 or f2 (above)


8/31/2011:

Len 40:
1 Parser, 2 states: 81.1
2 Parsers, EP, 2 states: 82.0 (might not have converged)
2 Parsers, ADF, 2 states, 82.5
1 Parser, 4 states: 83.9 (might not have converged)
1 Parser, 4 states, split featurization == independent bits: 84.4, 84.1, ****85.1, may not have converged

Len 15:

1 Parser, 2 states: 85.72
2 Parsers, EP, 2 states: 87.02
2 Parsers, ADF, 2 states: 87.59
1 Parser 4 states, split featurization: 87.77 
1 Parser, 4 states: 88.11


5/13/2011:

EP Versus Product:

Len 15, 2 2-state grammars:
1 Parser, viterbi: 85.1 (XXX check)
1 Parser, max rule: 85.7 (XXX hceck)
2 Parsers f0 viterbi: 86.0
2 Parsers, EP, max variational on last parser: 86.1 (XXX check)
2 Parsers, EP, max rule on last parser: 86.2
2 Parsers, Product, max variational: 86.3
2 Parserse, EP, max label on f0: 86.4
2 parsers, EP, max label, trained starting from good initialization: 86.5
2 Parsers, Product, max rule: 86.7



1/14/11:

4 state Stochastic discrim: 69 F1

seems low?

1/18/11:

Slav 3 splits, len 15: ~89F1
len 40: 87F1 (WTF)


1/23/11:
Len States F1
40   1    59.88  
40   2    67.22
40   4    71.97



15   1    75.44
15   2    89.76
15   4    82.5

1/25/11
40   2    72.18

Generative New RIGHT binarization: 64.75
Same with left.


1/28/11:
Lexicons:
Type: IncB:  F1:   Tag Accuracy:
Old   10     60.5     93.2
Unk1  10     60.5     93.2
Unk2  10     60.2     92.5
Unk3  10     59.9     92.1
Unk1   5     60.5     93.3
Unk1   2     60.6     93.3
Unk1   2     60.7     93.4


2/2/11:
current GenerativeTrainer
real    2m49.098s
user    8m7.350s
sys     0m14.754s

current SigTrainer:
real    2m53.907s
user    8m22.004s
sys     0m15.053s

post while loop:
real    2m8.360s
user    5m58.861s
sys     0m11.705s

