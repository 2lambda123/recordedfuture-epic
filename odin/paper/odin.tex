\documentclass[11pt,letter]{article}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{acl2012}
\usepackage{times}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage{amssymb}
\usepackage[small]{caption}
 \usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{comment}
\usepackage{enumerate}
\usepackage{latexsym}
\usepackage{tipa}
\usepackage{synttree}
\usepackage{color}
\newcommand{\an}[2]{#1\textrm{[}#2\textrm{]}}
\newcommand{\brule}[3]{ #1\to #2\,\,#3}
\newcommand{\blap}[1]{\smash[b]{\begin{tabular}[t]{@{}c@{}}#1\end{tabular}}}
\DeclareMathOperator*{\argmax}{arg\!max}
\DeclareMathOperator*{\argmin}{arg\!min}
\setlength\titlebox{4.5cm}    % Expanding the titlebox

\title{A joint model of nearly everything}

\author{David Hall  ~{\rm {and}} ~Dan Klein \\ Computer Science Division \\
University of California, Berkeley\\
\texttt{\{dlwh,klein\}@cs.berkeley.edu}}

\date{}

\begin{document}
\maketitle
\begin{abstract}

Guys, listen, we need to talk. We've been pussyfooting around this for long
enough. Annotate all the things. All of them.

\end{abstract}

\section{Introduction}


\section{The Model}

We now describe our model.  Its main purpose is to be awesome. 
XXX

The model divides into several pieces. We'll

\subsection{The Core}

The core of our model focuses on the words in each sentence, and any
"properties" they might have in the sentence. Each property is a
multinomial-random variable that may or may not be observed at test time. For
instance, words that might be nouns might have properties for their gender,
whether or not they are definite, if they are a kind of named entity, etc. In
addition, each word has a random variable for the constituent it governs,
including the span of the constituent and its label. Finally, each word has a
random variable for its parent, as in a dependency parse.

More specifically, for each word $w_{i}$, we have a collection of K property
random variables $p_{i1}, p_{i2}, \ldots, p_{iK}$. Each of the augmented models
will use some subset of these variables as part of its own inference procedure.
For instance, a named entity recognizer would use information about a word's
status as a named entity, along with perhaps information about the part of
speech tag and gender information. A parser might employ information about the
part of speech tags, but maintain ignorance with regard to named entities.


\subsection{Coreference Resolution}

The coreference component of our model is a pairwise, head-word-oriented model.
Specifically, each word in a document may be coreferent with some prior word.
Thus, the coreference model has, for each word $w_i$ in each document, a random
variable $ref_{i}$ taking values in ${0, 1, 2, \ldots, i-1}$ indicating what
that word's referent is.  We reserve 0 to be a dummy root of the document
value, indicating that the word has no referent. This model of coreference
induces a tree-structured coreference graph, not all that dissimilar to 
that of XXX.

The coreference component's primary job is to enforce coherence between the
properties of coreferent words. For instance, two words can usually be
coreferent only if they have the same gender and number. Thus, in inference,
the coreference

\bibliographystyle{acl}
\bibliography{refs}

\end{document}
